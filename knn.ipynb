{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions with k-nearest neighbors on the Iris Flowers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for the project\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and load the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "source": [
    "I split in 2 part the dataset (randomly), the first part is the training data and the second part will be use for the validation model\n",
    "I will use cross validation in order to compare the diffrence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in 2 the dataset \n",
    "def splitcsv(dataset):\n",
    "    validation_data = []\n",
    "    trainning_data = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        rd = random.choice([True, False])\n",
    "        if rd:\n",
    "            trainning_data.append(dataset[i]) \n",
    "        else:\n",
    "            validation_data.append(dataset[i])\n",
    "    return trainning_data, validation_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function from scratch :\n",
    "\n",
    "-Convert string column to float\n",
    "\n",
    "-Convert string column to integer\n",
    "\n",
    "-Find the min and max values for each column\n",
    "\n",
    "-Rescale dataset columns to the range 0-1 (normalization)\n",
    "\n",
    "-Calculate the Euclidean distance between two vectors\n",
    "\n",
    "-Locate the most similar neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        # print('[%s] => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "def distance_euclid(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "def find_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = distance_euclid(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "Make a prediction with neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(train, test_row, num_neighbors):\n",
    "    neighbors = find_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction"
   ]
  },
  {
   "source": [
    "Cross validation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(dataset,k):\n",
    "   # cross validation\n",
    "    for i in range(k):\n",
    "        # init dataset for training\n",
    "        # init dataset for testing\n",
    "        dataset_train = []\n",
    "        dataset_test = []\n",
    "        len_dataset = len(dataset)\n",
    "        # process the split\n",
    "        if i == (k-1):\n",
    "            dataset_test = dataset[int(len_dataset/4)*i:len_dataset]\n",
    "            dataset_train = dataset[0:int(len_dataset/4)*i-1]\n",
    "        else:\n",
    "            dataset_train = dataset\n",
    "            dataset_test = dataset[int(len_dataset/4)*i:int(len_dataset/4)*(i+1)]\n",
    "            for index in range(int(len_dataset/4)*i,int(len_dataset/4)*(i+1)):\n",
    "                dataset_train.pop(index)\n",
    "        \n",
    "        # define model parameter\n",
    "        num_neighbors = 4\n",
    "        # define the value for check the ratio of true positive by class\n",
    "        count_true = 0\n",
    "        count_true_1 =0\n",
    "        total_1 = 0\n",
    "        count_true_2 = 0\n",
    "        total_2 = 0\n",
    "        count_true_3 = 0\n",
    "        total_3 = 0\n",
    "        # predict the label\n",
    "        for j in range(0,len(dataset_test)):\n",
    "            label = 0\n",
    "            label = prediction(dataset_train, dataset_test[j][0:3], num_neighbors)\n",
    "            # precision\n",
    "            if dataset_test[j][4] == 0:\n",
    "                    total_1 = total_1 + 1\n",
    "            if dataset_test[j][4] == 1:\n",
    "                    total_2 = total_2 + 1\n",
    "            if dataset_test[j][4] == 2:\n",
    "                    total_3 = total_3 + 1\n",
    "            # validation \n",
    "            if dataset_test[j][4] == label:\n",
    "                count_true = count_true + 1\n",
    "                if label == 0:\n",
    "                    count_true_1 = count_true_1 + 1\n",
    "                elif label == 1:\n",
    "                    count_true_2 = count_true_2 + 1\n",
    "                elif label == 2:\n",
    "                    count_true_3 = count_true_3 + 1\n",
    "            \n",
    "        pourcentage_tot = count_true/len(dataset_test)\n",
    "        pourcentage_1 = count_true_1/total_1\n",
    "        pourcentage_2 = count_true_2/total_2\n",
    "        pourcentage_3 = count_true_3/total_3\n",
    "        print('K=%d partition n=%d'% (k,i))\n",
    "        print('there are %d true over %d prediction ' % (count_true,len(dataset_test)))\n",
    "        print('CLASS 0 , there are %s true over %s prediction ' % (count_true_1,total_1))\n",
    "        print('CLASS 1 , there are %s true over %s prediction ' % (count_true_2,total_2))\n",
    "        print('CLASS 2 , there are %s true over %s prediction ' % (count_true_3,total_3))\n",
    "        print('Pourcentage CLASS 0 ==> %s' % (pourcentage_1 ))\n",
    "        print('Pourcentage CLASS 1 ==> %s' % (pourcentage_2 ))\n",
    "        print('Pourcentage CLASS 2 ==> %s' % (pourcentage_3 ))\n",
    "        print('Pourcentage TOTAL ==> %s' % (pourcentage_tot ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction with KNN on Iris Dataset prediction \n",
    "\n",
    "class 0 is Iris-setosa\n",
    "\n",
    "class 1 is Iris-versicolor\n",
    "\n",
    "class 2 is Iris-virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K=1 partition n=0\nthere are 125 true over 150 prediction \nCLASS 0 , there are 49 true over 50 prediction \nCLASS 1 , there are 44 true over 50 prediction \nCLASS 2 , there are 32 true over 50 prediction \nPourcentage CLASS 0 ==> 0.98\nPourcentage CLASS 1 ==> 0.88\nPourcentage CLASS 2 ==> 0.64\nPourcentage TOTAL ==> 0.8333333333333334\nK=2 partition n=0\nthere are 25 true over 37 prediction \nCLASS 0 , there are 8 true over 8 prediction \nCLASS 1 , there are 12 true over 16 prediction \nCLASS 2 , there are 5 true over 13 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.75\nPourcentage CLASS 2 ==> 0.38461538461538464\nPourcentage TOTAL ==> 0.6756756756756757\nK=2 partition n=1\nthere are 72 true over 85 prediction \nCLASS 0 , there are 36 true over 36 prediction \nCLASS 1 , there are 19 true over 24 prediction \nCLASS 2 , there are 17 true over 25 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.7916666666666666\nPourcentage CLASS 2 ==> 0.68\nPourcentage TOTAL ==> 0.8470588235294118\nK=3 partition n=0\nthere are 23 true over 28 prediction \nCLASS 0 , there are 7 true over 7 prediction \nCLASS 1 , there are 8 true over 10 prediction \nCLASS 2 , there are 8 true over 11 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.8\nPourcentage CLASS 2 ==> 0.7272727272727273\nPourcentage TOTAL ==> 0.8214285714285714\nK=3 partition n=1\nthere are 18 true over 21 prediction \nCLASS 0 , there are 9 true over 9 prediction \nCLASS 1 , there are 6 true over 7 prediction \nCLASS 2 , there are 3 true over 5 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.8571428571428571\nPourcentage CLASS 2 ==> 0.6\nPourcentage TOTAL ==> 0.8571428571428571\nK=3 partition n=2\nthere are 29 true over 32 prediction \nCLASS 0 , there are 16 true over 16 prediction \nCLASS 1 , there are 6 true over 8 prediction \nCLASS 2 , there are 7 true over 8 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.75\nPourcentage CLASS 2 ==> 0.875\nPourcentage TOTAL ==> 0.90625\nK=4 partition n=0\nthere are 13 true over 16 prediction \nCLASS 0 , there are 4 true over 4 prediction \nCLASS 1 , there are 3 true over 3 prediction \nCLASS 2 , there are 6 true over 9 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 1.0\nPourcentage CLASS 2 ==> 0.6666666666666666\nPourcentage TOTAL ==> 0.8125\nK=4 partition n=1\nthere are 11 true over 12 prediction \nCLASS 0 , there are 6 true over 6 prediction \nCLASS 1 , there are 4 true over 5 prediction \nCLASS 2 , there are 1 true over 1 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.8\nPourcentage CLASS 2 ==> 1.0\nPourcentage TOTAL ==> 0.9166666666666666\nK=4 partition n=2\nthere are 7 true over 9 prediction \nCLASS 0 , there are 4 true over 4 prediction \nCLASS 1 , there are 2 true over 3 prediction \nCLASS 2 , there are 1 true over 2 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 0.6666666666666666\nPourcentage CLASS 2 ==> 0.5\nPourcentage TOTAL ==> 0.7777777777777778\nK=4 partition n=3\nthere are 9 true over 9 prediction \nCLASS 0 , there are 6 true over 6 prediction \nCLASS 1 , there are 2 true over 2 prediction \nCLASS 2 , there are 1 true over 1 prediction \nPourcentage CLASS 0 ==> 1.0\nPourcentage CLASS 1 ==> 1.0\nPourcentage CLASS 2 ==> 1.0\nPourcentage TOTAL ==> 1.0\n"
     ]
    }
   ],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "dataset_shuffle =[]\n",
    "for j in range(len(dataset[0])-1):\n",
    "\tconvert_str_column_to_float(dataset, j)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "dataset_shuffle = random.sample(dataset, len(dataset))\n",
    "for k in range(5):\t\n",
    "\tcross_validation(dataset_shuffle,k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d65f8af9b1347936d5c0a715a1a101b7602968bee42a1bc2161adfc924f1cbb0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}